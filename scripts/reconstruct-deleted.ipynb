{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import pytesseract\n",
    "from fuzzysearch import find_near_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ==================================================================================================\n",
    "    Move only slide/long screenshots\n",
    "    ==================================================================================================\n",
    "'''\n",
    "\n",
    "# move only slide/long screenshots\n",
    "potential_img = 'potential'\n",
    "img_dir = 'imgs'\n",
    "for img_path in glob.glob(os.path.join(potential_img, \"*.jpg\")):\n",
    "    img = Image.open(img_path)\n",
    "    w, h = img.size\n",
    "    img.close()\n",
    "    max_aspect_ratio = 21/9 \n",
    "    ascpect_ratio = h / w\n",
    "    if ascpect_ratio > max_aspect_ratio:\n",
    "        print(f'screenshot: {img_path}')\n",
    "        shutil.move(img_path, os.path.join(img_dir, os.path.basename(img_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ==================================================================================================\n",
    "    OCR all screenshots in imgs directory and save them as txt files in strs directory\n",
    "    ==================================================================================================\n",
    "'''\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def load_screenshot(img_path: os.PathLike) -> str:\n",
    "        img = Image.open(img_path)\n",
    "        try:\n",
    "            ocr_text = pytesseract.image_to_string(img, lang='eng+pol')\n",
    "        except pytesseract.TesseractError as e:\n",
    "            # Cut the image in half\n",
    "            width, height  = img.size\n",
    "            img_top = img.crop((0,0, width, height//2))\n",
    "            img_down = img.crop((0, height//2, width, height))\n",
    "            ocr_text = pytesseract.image_to_string(img_top, lang='eng+pol')\n",
    "            ocr_text += pytesseract.image_to_string(img_down, lang='eng+pol')\n",
    "        return ocr_text\n",
    "\n",
    "img_dir = 'imgs'\n",
    "str_dir = 'strs'\n",
    "for img_path in tqdm(glob.glob(os.path.join(img_dir, \"*.jpg\"))):\n",
    "    if os.path.exists(os.path.join(str_dir, Path(img_path).stem + \".txt\")):\n",
    "        continue\n",
    "    \n",
    "    ocr_str = load_screenshot(img_path)\n",
    "    str_path = os.path.join(str_dir, Path(img_path).stem + \".txt\")\n",
    "    with open(str_path, \"w\", encoding='UTF-8') as file:\n",
    "        file.write(ocr_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNACCESSIBLE_STATUS = [\" is a private video\", \" is unavailable\"]\n",
    "CANT_BACKUP_STATUS = [\"Remote end closed connection without response\", \"IncompleteRead\", 'streamingData', \" is age restricted, and can't be accessed without logging in.\", \" The read operation timed out\", \" HTTP Error 403: Forbidden\", \" The read operation timed out\"] #, \" is age restricted, and can't be accessed without logging in.\", \n",
    "DOWNLOADED_STATUS = [\"Downloaded\"]\n",
    "TITLE_DELETED_STATUS =  [\"Deleted video\", \"Private video\", \"This video is unavailable.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANIFEST_PATH = r'C:\\Users\\lukas\\Desktop\\youtube-autobackup\\data\\manifest.h5'\n",
    "df_output = pd.read_hdf(MANIFEST_PATH, key=\"df\")\n",
    "df_output['second_id'] = ''\n",
    "df_output.to_hdf(os.path.join(os.path.dirname(MANIFEST_PATH), 'recreated.h5'), key=\"df\", mode='w')\n",
    "MANIFEST_PATH = r'C:\\Users\\lukas\\Desktop\\youtube-autobackup\\data\\recreated.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "HISTORY_PATH = r'C:\\Users\\lukas\\Desktop\\youtube-autobackup\\data\\history.json'\n",
    "TITLE_DELETED_STATUS =  [\"Deleted video\", \"Private video\", \"This video is unavailable.\"]\n",
    "\n",
    "with open(HISTORY_PATH, 'r', encoding='UTF-8') as file:\n",
    "    history = json.load(file)\n",
    "\n",
    "history_df = pd.json_normalize(history)\n",
    "history_df = history_df.dropna(subset=['titleUrl'])\n",
    "history_df['video_id'] = history_df['titleUrl'].apply(lambda x: x.split('?v=')[1])\n",
    "history_df['title'] = history_df['title'].apply(lambda x: x.replace('Obejrzano: ', ''))\n",
    "history_df = history_df[~history_df['title'].str.contains('https://www.youtube.com/watch')]\n",
    "history_df = history_df[['video_id', 'title']]\n",
    "history_df = history_df.drop_duplicates(subset=['video_id'])\n",
    "history_df = history_df.set_index('video_id')\n",
    "\n",
    "df_output = pd.read_hdf(MANIFEST_PATH, key=\"df\")\n",
    "df_output = df_output.set_index('contentDetails.videoId')\n",
    "df_output = df_output.join(history_df, how='left', rsuffix='_history')\n",
    "df_output['snippet.title'] = df_output[['snippet.title', 'title']].apply(lambda x: x['title'] if isinstance(x['snippet.title'], str) and any([keyword in x['snippet.title'] for keyword in TITLE_DELETED_STATUS]) else x['snippet.title'], axis=1)\n",
    "# df_output['truetable'] = df_output[['snippet.title', 'title']].apply(lambda x: True if any([keyword in x['snippet.title'] for keyword in TITLE_DELETED_STATUS]) else False, axis=1)\n",
    "# df_output = df_output[df_output['truetable']]\n",
    "df_output = df_output.drop(columns=['title'])\n",
    "df_output = df_output.reset_index()\n",
    "df_output.to_hdf(MANIFEST_PATH, key=\"df\", mode='w')\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.read_hdf(MANIFEST_PATH, key=\"df\")\n",
    "df_output['snippet.title'] = df_output['snippet.title'].fillna('')\n",
    "df_output.to_hdf(MANIFEST_PATH, key=\"df\", mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "TITLE_DELETED_STATUS =  [\"Deleted video\", \"Private video\", \"This video is unavailable.\"]\n",
    "OCR_PATH = r'C:\\Users\\lukas\\Desktop\\youtube-autobackup\\scripts\\strs\\*.txt'\n",
    "MANIFEST_PATH = r'C:\\Users\\lukas\\Desktop\\youtube-autobackup\\data\\recreated.h5'\n",
    "\n",
    "\n",
    "def clip(num, _min, _max):\n",
    "    if num > _max:\n",
    "        return _max\n",
    "    if num < _min:\n",
    "        return min\n",
    "    return num\n",
    "\n",
    "def get_context(idx, df, pos):\n",
    "    found_idx = None\n",
    "    while found_idx is None:\n",
    "        if pos == 'after':\n",
    "            idx += 1\n",
    "        elif pos == 'before':\n",
    "            idx -= 1\n",
    "        \n",
    "        if idx > len(df) or idx < 0:\n",
    "            found_idx = clip(idx, 0, len(df))\n",
    "        elif not any([key_word in df.loc[idx][\"backup_status\"] for key_word in UNACCESSIBLE_STATUS] + [df.loc[idx][\"snippet.videoOwnerChannelId\"] == os.getenv(\"USER_ID\")]):\n",
    "            found_idx = idx\n",
    "        \n",
    "    return df.loc[found_idx]\n",
    "\n",
    "UNACCESSIBLE_STATUS = [\" is a private video\", \" is unavailable\"]\n",
    "\n",
    "loaded_h5 = pd.read_hdf(MANIFEST_PATH, key=\"df\")\n",
    "playlists_ids = loaded_h5['playlist_id'].unique()\n",
    "playlists_ids = ['PLomXEcQ9kTsF_XwsVGjICmnpm7ctX3hXH']\n",
    "\n",
    "for playlists_id in playlists_ids:\n",
    "    playlists_id = playlists_id\n",
    "    break\n",
    "\n",
    "def deleted_song_context_gen(loaded_h5, playlists_id):\n",
    "    loaded_h5 = loaded_h5.reset_index(drop=True)\n",
    "\n",
    "    subset_df = loaded_h5.copy()\n",
    "    subset_df = loaded_h5[loaded_h5['playlist_id'] == playlists_id]\n",
    "    subset_df = subset_df.sort_values(by='snippet.position')\n",
    "    subset_df = subset_df.reset_index(drop=True)\n",
    "    subset_df.to_csv('output.csv')\n",
    "\n",
    "    for idx, playlist_row in subset_df.iterrows():\n",
    "        if not any([key_word in playlist_row[\"backup_status\"] for key_word in UNACCESSIBLE_STATUS] + [playlist_row[\"snippet.videoOwnerChannelId\"] == os.getenv(\"USER_ID\")]):\n",
    "            continue\n",
    "        \n",
    "        print('=================================')\n",
    "        print(playlist_row['snippet.title'])\n",
    "        title_exist = False if any([keyword in playlist_row['snippet.title'] for keyword in TITLE_DELETED_STATUS]) or playlist_row['snippet.title']==\"\" else True\n",
    "# isinstance(playlist_row['snippet.title'], float) or \n",
    "        if title_exist:\n",
    "            print(f\"{title_exist} - {idx}: ({playlist_row['snippet.position']}) {playlist_row['snippet.resourceId.videoId']} | {playlist_row['snippet.title']} | {playlist_row['snippet.videoOwnerChannelTitle']} ({playlist_row['backup_status']})\")\n",
    "            yield None, playlist_row, None\n",
    "        else:\n",
    "            context_before = get_context(idx, subset_df, pos='before')\n",
    "            context_after = get_context(idx, subset_df, pos='after')\n",
    "\n",
    "            print(f'''{title_exist} - {idx - 1}: ({context_before['snippet.position']}) {context_before['snippet.resourceId.videoId']} | {context_before['snippet.title']} | {context_before['snippet.videoOwnerChannelTitle']} ({context_before['backup_status']})\n",
    "    {title_exist} - {idx}: ({playlist_row['snippet.position']}) {playlist_row['snippet.resourceId.videoId']} | {playlist_row['snippet.title']} | {playlist_row['snippet.videoOwnerChannelTitle']} ({playlist_row['backup_status']})\n",
    "    {title_exist} - {idx + 1}: ({context_after['snippet.position']}) {context_after['snippet.resourceId.videoId']} | {context_after['snippet.title']} | {context_after['snippet.videoOwnerChannelTitle']} ({context_after['backup_status']})''')\n",
    "            yield context_before, playlist_row, context_after\n",
    "\n",
    "CONTEXT_LEN = 20\n",
    "from fuzzysearch import find_near_matches\n",
    "\n",
    "def parse_ocr(ocr: str, start_str: str, end_str: str):\n",
    "    # start_idx = ocr.find(start_str)\n",
    "    start_idx = find_near_matches(start_str, ocr, max_l_dist=1)\n",
    "    if start_idx == []:\n",
    "        return None\n",
    "    start_idx = start_idx[0]\n",
    "    start_idx = start_idx.start\n",
    "    start_idx = 0 if start_idx - CONTEXT_LEN < 0 else start_idx - CONTEXT_LEN\n",
    "    # end_idx = ocr.find(end_str)\n",
    "    end_idx = find_near_matches(end_str, ocr, max_l_dist=1)\n",
    "    # print(end_idx, flush=True)\n",
    "    if end_idx == []:\n",
    "        end_idx = len(ocr)\n",
    "    else:\n",
    "        end_idx = end_idx[0]\n",
    "        end_idx = end_idx.start\n",
    "    end_idx = len(ocr) if end_idx + CONTEXT_LEN >= len(ocr) else end_idx + CONTEXT_LEN\n",
    "    \n",
    "    return ocr[start_idx:end_idx]\n",
    "\n",
    "def get_id_input(df, _id):\n",
    "    df_subset = df[df['snippet.resourceId.videoId'] == _id]\n",
    "\n",
    "    if df_subset.iloc[0]['second_id'] != \"\":\n",
    "        return df\n",
    "\n",
    "    input_id = input('ID: ')\n",
    "    if input_id.lower() == \"exit\":\n",
    "        raise KeyboardInterrupt\n",
    "    elif input_id:\n",
    "        df.loc[df['snippet.resourceId.videoId'] == _id,'second_id'] = input_id\n",
    "    df.to_hdf(MANIFEST_PATH, key=\"df\")\n",
    "    return df\n",
    "\n",
    "deleted_context = deleted_song_context_gen(loaded_h5=loaded_h5, playlists_id=playlists_id)\n",
    "for context_before, playlist_row, context_after in deleted_context:\n",
    "    print('', flush=True)\n",
    "    if context_after is None and context_before is None:\n",
    "        # continue\n",
    "        try:\n",
    "            loaded_h5 = get_id_input(loaded_h5, playlist_row['snippet.resourceId.videoId'])\n",
    "        except KeyboardInterrupt:\n",
    "            loaded_h5.to_hdf(MANIFEST_PATH, key=\"df\")\n",
    "            raise\n",
    "        continue\n",
    "    \n",
    "    for txt_path in glob.glob(OCR_PATH):\n",
    "        with open(txt_path, 'r', encoding='UTF-8') as file:\n",
    "            txt_ocr = file.read()\n",
    "        text = parse_ocr(txt_ocr, context_before['snippet.title'], context_after['snippet.title'])\n",
    "        if text is not None:\n",
    "            print(text, flush=True)\n",
    "            print('--------------------------------------------', flush=True)\n",
    "            try:\n",
    "                loaded_h5 = get_id_input(loaded_h5, playlist_row['snippet.resourceId.videoId'])\n",
    "            except KeyboardInterrupt:\n",
    "                loaded_h5.to_hdf(MANIFEST_PATH, key=\"df\")\n",
    "                raise\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANIFEST_PATH = r'C:\\Users\\lukas\\Desktop\\youtube-autobackup\\data\\recreated_sinus.h5'\n",
    "df = pd.read_hdf(MANIFEST_PATH, key=\"df\")\n",
    "df = df[df['second_id'] != '']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "========================================\n",
    "    Re-enter deleted playlists\n",
    "========================================\n",
    "'''\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from youtube_manager import YoutubeManager\n",
    "\n",
    "RECREATED_PATH = r\"C:\\Users\\lukas\\Desktop\\youtube-autobackup\\data\\recreated_*.h5\"\n",
    "recreated_data = glob.glob(RECREATED_PATH)\n",
    "\n",
    "recreated_data = pd.concat([pd.read_hdf(file, key=\"df\") for file in recreated_data])\n",
    "recreated_data = recreated_data[(recreated_data[\"second_id\"] != \"\") & (recreated_data[\"second_id\"] != \"NULL\")]\n",
    "print(len(recreated_data), \"==\", len(recreated_data.drop_duplicates(subset=[\"second_id\", \"contentDetails.videoId\"])))\n",
    "\n",
    "\n",
    "yt_manager = YoutubeManager(token_path=r'C:\\Users\\lukas\\Desktop\\youtube-autobackup\\data\\token.json')\n",
    "all_videos = {}\n",
    "all_playlists = yt_manager.playlist_list(os.getenv(\"USER_ID\"))\n",
    "\n",
    "for playlist in all_playlists:\n",
    "    playlist_id = playlist[\"id\"]\n",
    "    playlist_vids = yt_manager.playlist_elements(playlist_id)\n",
    "    all_videos[playlist_id] = playlist_vids\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for playlist_id, playlist_vids in all_videos.items():\n",
    "    batch = pd.json_normalize(playlist_vids)\n",
    "    batch[\"playlist_id\"] = playlist_id\n",
    "    df = pd.concat([df, batch])\n",
    "\n",
    "join_df = recreated_data.join(df.set_index(\"contentDetails.videoId\"), on=\"contentDetails.videoId\", rsuffix=\"_fetched\")\n",
    "join_df = join_df[[\"contentDetails.videoId\", \"second_id\", \"id_fetched\", \"snippet.position_fetched\", \"playlist_id_fetched\"]]\n",
    "join_df = join_df.dropna(subset=[\"snippet.position_fetched\"])\n",
    "\n",
    "added_id = ['vb9hgfngIbk']\n",
    "\n",
    "for index, row in join_df.iterrows():\n",
    "    if row['second_id'] in added_id:\n",
    "        continue\n",
    "    print(f\"Adding {row['second_id']} for {row['contentDetails.videoId']} at position {row['snippet.position_fetched']} for playlist {row['playlist_id_fetched']}\")\n",
    "    yt_manager.playlistItems_insert(\n",
    "        row[\"playlist_id_fetched\"],\n",
    "        row[\"second_id\"],\n",
    "        position=row[\"snippet.position_fetched\"],\n",
    "    )\n",
    "    yt_manager.playlistItems_delete(row[\"id_fetched\"])\n",
    "    added_id.append(row[\"second_id\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
